---
title: "Simulation to explore raking"
author: "Evan L. Ray"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message = FALSE, cache = TRUE)
```

# Introduction

In this document, I conduct a small simulation study to investigate the use of raking to correct for biased sampling.

We will investigate the use of raking with a single covariate, the location (MA county) where a sample was collected.

# Setup

```{r}
library(tidyverse)
library(extraDistr)
library(anesrake)
library(weights)
library(survey)
```

We obtained the following population data for MA counties as of 2023 from the US Census:
```{r}
# load in data with population per county in MA
pops <- readr::read_csv("ma_county_populations.csv") |>
  dplyr::filter(location != "Massachusetts") |>
  dplyr::select(location, pop = `2023`) |>
  dplyr::mutate(
    location = stringr::str_extract(location, "^.+?\\ "),
    location = substr(location, 1, nchar(location) - 1),
    prop_pop = pop / sum(pop)
  )
pops
```

Here we generate a simulated "population" of infected people in each county, based on the following assumptions:
- In all counties, the infection rate is 100/100,000.
- Each county has a different mix of clade proportions, roughly centered around 10% clade A, 30% clade B, 60% clade C

```{r}
# set up a "population" of infected people
set.seed(42)
mean_clade_props <- c(0.1, 0.3, 0.6)
n_clades <- length(mean_clade_props)
clade_names <- LETTERS[seq_along(mean_clade_props)]
clade_props_by_county <- rdirichlet(alpha = mean_clade_props * 100,
                                    n = nrow(pops)) |>
  `colnames<-`(clade_names) |>
  as.data.frame()

# for all counties,
# assume 5/100,000 hospitalization rate
# and 5% infection/hospitalization rate, i.e. 20 infections per hospitalization
# gets us to a 100/100,000 infection rate
pops$inf_count <- pops$pop * 5 / 100000 * 20
clade_inf_by_county <- round(clade_props_by_county * pops$inf_count)
print(cbind(pops["location"], clade_inf_by_county))

# the "realized" clade proportions in each county:
print(cbind(pops["location"],
            sweep(clade_inf_by_county,
                  1,
                  apply(clade_inf_by_county, 1, sum),
                  "/")))
```

Our goal is to obtain estimates of the proportion of all infections that are from each clade, state-wide:
```{r}
# population proportions to estimate
clade_counts <- apply(clade_inf_by_county, 2, sum)
clade_props <- clade_counts / sum(clade_counts)
print(clade_props)
```

Now we draw a sample of 100 infections, with the following assumptions:

- About 80% of the samples come from Worcester county and the other counties contribute samples with probability proportional to their population size.
- Within each county, any infected individual has equal probability of being selected for sequencing.
```{r}
# draw a sample of 100 infections
# more samples from Worcester county; other counties proportional to size
# equal probability of observing any infected person
# (e.g., not more likely to observe some variants than others)
sample_size <- 100
sampling_prob_by_county <- data.frame(
  location = c("Barnstable", "Berkshire", "Bristol", "Dukes", "Essex",
               "Franklin", "Hampden", "Hampshire", "Middlesex",
               "Nantucket", "Norfolk", "Plymouth", "Suffolk", "Worcester")
) |>
  dplyr::left_join(
    pops |>
      dplyr::select(location, pop) |>
      dplyr::filter(location != "Worcester") |>
      dplyr::mutate(rel_pop = pop / sum(pop)),
    by = "location"
  ) |>
  dplyr::mutate(
    sampling_prob = ifelse(
      location == "Worcester",
      0.8,
      0.2 * rel_pop
    )
  )

# draw the sample:
# first, select location with probabilities found above
# then, select individuals
sample_counts_by_county <- rmultinom(
  n = 1,
  size = sample_size,
  prob = sampling_prob_by_county$sampling_prob
)[, 1]

sample_one_county <- function(i, count, clade_inf_by_county) {
  infs <- rep(clade_names, times = clade_inf_by_county[i, ])
  if (count > length(infs)) {
    stop("count too large!")
  }

  sample(infs, size = count) |>
    factor(levels = clade_names) |>
    table() |>
    unname()
}

samples_by_county <- purrr::map2(
  seq_along(sample_counts_by_county), sample_counts_by_county,
  function(i, count) {
    matrix(sample_one_county(i, count, clade_inf_by_county), nrow = 1)
  }
)
samples_by_county <- do.call(rbind, samples_by_county)
colnames(samples_by_county) <- clade_names
print(cbind(pops["location"], as.data.frame(samples_by_county)))

# get to a line list format for sequences, needed for some analyses below
samples_linelist <- samples_by_county |>
  as.data.frame() |>
  dplyr::mutate(location = pops$location) |>
  tidyr::pivot_longer(
    cols = dplyr::all_of(clade_names),
    names_to = "clade",
    values_to = "count"
  ) |>
  dplyr::filter(count > 0) |>
  dplyr::group_by(location, clade) |>
  dplyr::mutate(
    clade = list(rep(clade, count))
  ) |>
  dplyr::ungroup() |>
  dplyr::select(-count) |>
  tidyr::unnest(clade) |>
  dplyr::mutate(
    caseid = row_number()#,
    # location = factor(location, levels = pops$location)
  )
head(samples_linelist)
```

Now, we compute two kind of estimates:

1. A naive estimate, obtained as the proportion of each variant in our sample, aggregating across all counties
2. Estimates based on raking, which downweight samples from Worcester county and upweight samples from other counties

```{r}
# naive estimate: aggregate across counties, compute proportion
naive_est <- apply(samples_by_county, 2, sum)
naive_est <- naive_est / sum(naive_est)
naive_est <- data.frame(
  clade = clade_names,
  estimate = naive_est
)
print(naive_est)
```

We can also obtain confidence intervals for these proportions through a variety of methods. Here's one option, from [this stackoverflow post](https://stackoverflow.com/questions/76320119/is-there-a-way-to-estimate-multinomial-proportions-with-confidence-intervals-usi) (this may not be the best approach?):
```{r}
dat_design <- suppressWarnings(svydesign(~ 1, data = samples_linelist)) # convert data to survey design

f <- function(clade, design) {
  est <- eval(bquote(
    svyciprop(~ I(clade == .(clade)), design, method = "xlogit")
  ))
  result <- data.frame(
    clade = clade,
    estimate = as.numeric(est),
    ci_lower = attr(est, "ci")[1],
    ci_upper = attr(est, "ci")[2]
  )
  row.names(result) <- 1L
  return(result)
}
est_with_ci_naive <- purrr::map(clade_names, f, design = dat_design) |>
  purrr::list_rbind()
```

And we can obtain bootstrap confidence intervals as follows:
```{r}
dat_design_bs <- as.svrepdesign(dat_design, type = "bootstrap")
est_with_ci_bs <- purrr::map(clade_names, f, design = dat_design_bs) |>
  purrr::list_rbind()
row.names(est_with_ci_bs) <- NULL
```

Now, with raking:
```{r}
# our target weighting per county: proportional to total infection count
inf_prop_by_county <- data.frame(
  location = pops$location,
  inf = apply(clade_inf_by_county, 1, sum)
) |>
  dplyr::filter(location %in% unique(samples_linelist$location)) |>
  dplyr::mutate(
    inf_prop = inf / sum(inf)
  )
targets_survey <- list(
  inf_prop_by_county |>
    dplyr::select(location, Freq = inf) |>
    as.data.frame()
)
print(targets_survey)

dat_design_raked <- rake(
  dat_design,
  sample.margins = list(~ location),
  population.margins = targets_survey
)

# how many population members is each person in the sample accounting for?
attr(dat_design_raked$postStrata[[1]][[1]], "weights")

# Another method that didn't work in my first try
# I think this might work with better choices for bounds and trim, though.

# dat_design_raked <- calibrate(
#   dat_design,
#   formula = ~ location, # list of formulas specifying raking variables
# #   population = xtabs(~location, samples_linelist),
#   population = targets$location, # list of data frames showing desired counts
#   bounds = c(0.1, 10),
#   trim = c(0.2, 5),
#   force = TUE
# )

est_with_ci_raked <- purrr::map(clade_names, f, design = dat_design_raked) |>
  purrr::list_rbind()
```

The below tries out a different package, which allows us to specify a limit on the magnitude of the weights.  We'll see why this matters below.

```{r}
# raking-based estimate using the anesrake package

# our target weighting per county: proportional to total infection count
targets_anesrake <- list(
  location = inf_prop_by_county$inf_prop
)
names(targets_anesrake$location) <- inf_prop_by_county$location
print(targets_anesrake)

# estimate weights by raking, with a maximum weight of 5
raking_output_cap5 <- anesrake(
  inputter = targets_anesrake,
  dataframe = as.data.frame(samples_linelist |> dplyr::mutate(location = factor(location))),
  caseid = samples_linelist$caseid,
  verbose = FALSE,
  cap = 5,
  choosemethod = "total",
  type = "pctlim",
  pctlim = .05,
  nlim = 5,
  iterate = TRUE,
  force1 = TRUE
)

# the weights assigned to observations from each county:
cbind(samples_linelist, raking_output_cap5["weightvec"]) |>
  dplyr::distinct(location, weightvec)

# average weight across all observations in the sample is 1
mean(raking_output_cap5$weightvec)

# compute the raking estimate as a weighted sample proportion
# this package does not provide the option to get interval estimates
raking_est_cap5 <- data.frame(
  clade = clade_names,
  est = wpct(samples_linelist$clade, raking_output_cap5$weightvec)
)

# estimate weights by raking, with a maximum weight of 2
raking_output_cap2 <- anesrake(
  inputter = targets_anesrake,
  dataframe = as.data.frame(samples_linelist |> dplyr::mutate(location = factor(location))),
  caseid = samples_linelist$caseid,
  verbose = FALSE,
  cap = 2,
  choosemethod = "total",
  type = "pctlim",
  pctlim = .05,
  nlim = 5,
  iterate = TRUE,
  force1 = TRUE
)

raking_est_cap2 <- data.frame(
  clade = clade_names,
  est = wpct(samples_linelist$clade, raking_output_cap2$weightvec)
)

# compare naive and raking estimates to population values
print(est_with_ci_naive)
print(est_with_ci_bs)
print(raking_est_cap2)
print(raking_est_cap5)
print(est_with_ci_raked)
print(clade_props)
```

# Estimates across repeated samples

## Simulation 1: overrepresented county similar to overall population

We keep the simulated "population" of infected people above fixed, and investigate performance of the naive and raking-based estimators across repeated samples of cases from that population to select for sequencing. The sampling mechanism is the same as above (100 sequences, about 80% from Worcester county, the remaining coming from other counties proportional to size).

```{r, results='hide'}
draw_samples_linelist <- function(clade_inf_by_county) {
  sample_counts_by_county <- rmultinom(
    n = 1,
    size = sample_size,
    prob = sampling_prob_by_county$sampling_prob
  )[, 1]

  samples_by_county <- purrr::map2(
    seq_along(sample_counts_by_county), sample_counts_by_county,
    function(i, count) {
      matrix(sample_one_county(i, count, clade_inf_by_county), nrow = 1)
    }
  )
  samples_by_county <- do.call(rbind, samples_by_county)
  colnames(samples_by_county) <- clade_names

  # get to a line list format for sequences
  samples_linelist <- samples_by_county |>
    as.data.frame() |>
    dplyr::mutate(location = pops$location) |>
    tidyr::pivot_longer(
      cols = dplyr::all_of(clade_names),
      names_to = "clade",
      values_to = "count"
    ) |>
    dplyr::filter(count > 0) |>
    dplyr::group_by(location, clade) |>
    dplyr::mutate(
      clade = list(rep(clade, count))
    ) |>
    dplyr::ungroup() |>
    dplyr::select(-count) |>
    tidyr::unnest(clade) |>
    dplyr::mutate(
      caseid = row_number(),
      location = factor(location, levels = pops$location)
    )

  return(samples_linelist)
}

get_est_anesrake <- function(inf_prop_by_county, samples_linelist, cap) {
  targets <- list(
    location = inf_prop_by_county$inf_prop
  )
  names(targets$location) <- inf_prop_by_county$location

  # estimate weights by raking
  raking_output <- anesrake(
    inputter = targets,
    dataframe = as.data.frame(samples_linelist |> dplyr::mutate(location = factor(location))),
    caseid = samples_linelist$caseid,
    verbose = FALSE,
    cap = cap,
    choosemethod = "total",
    type = "pctlim",
    pctlim = .05,
    nlim = 5,
    iterate = TRUE,
    force1 = TRUE
  )

  # compute the raking estimate as a weighted sample proportion
  result <- data.frame(
    clade = clade_names,
    estimate = wpct(samples_linelist$clade, raking_output$weightvec),
    method = paste0("anesrake_cap", cap)
  )

  return(result)
}

get_est_survey <- function(inf_prop_by_county, samples_linelist, bootstrap=FALSE, rake=FALSE) {
  method <- "naive"
  dat_design <- suppressWarnings(svydesign(~ 1, data = samples_linelist)) # convert data to survey design

  if (bootstrap) {
    method <- "naive_bs"
    dat_design <- as.svrepdesign(dat_design, type = "bootstrap")
  }

  if (rake) {
    method <- "rake"
    targets <- list(
      inf_prop_by_county |>
        dplyr::filter(location %in% unique(samples_linelist$location)) |>
        dplyr::select(location, Freq = inf) |>
        as.data.frame()
    )

    dat_design <- rake(
      dat_design,
      sample.margins = list(~ location),
      population.margins = targets
    )
  }

  f <- function(clade, design) {
    est <- eval(bquote(
      svyciprop(~ I(clade == .(clade)), design, method = "xlogit")
    ))
    result <- data.frame(
      clade = clade,
      estimate = as.numeric(est),
      ci_lower = attr(est, "ci")[1],
      ci_upper = attr(est, "ci")[2]
    )
    row.names(result) <- 1L
    return(result)
  }

  est_with_ci <- purrr::map(clade_names, f, design = dat_design) |>
    purrr::list_rbind() |>
    dplyr::mutate(
      method = method
    )

  return(est_with_ci)
}

do_one_replicate <- function(i, clade_inf_by_county) {
  samples_linelist <- draw_samples_linelist(clade_inf_by_county)

  inf_prop_by_county <- data.frame(
    location = pops$location,
    inf = apply(clade_inf_by_county, 1, sum)
  ) |>
    dplyr::filter(location %in% unique(samples_linelist$location)) |>
    dplyr::mutate(
      inf_prop = inf / sum(inf)
    )

  # naive estimate: aggregate across counties, compute proportion
  result <- dplyr::bind_rows(
    get_est_anesrake(inf_prop_by_county, samples_linelist, cap = 2),
    get_est_anesrake(inf_prop_by_county, samples_linelist, cap = 5),
    get_est_survey(inf_prop_by_county, samples_linelist, bootstrap=FALSE, rake=FALSE),
    # get_est_survey(inf_prop_by_county, samples_linelist, bootstrap=TRUE, rake=FALSE),
    get_est_survey(inf_prop_by_county, samples_linelist, bootstrap=FALSE, rake=TRUE)
  )

  return(result)
}

n_replicates <- 1000
set.seed(42)
sim_results <- purrr::map(seq_len(n_replicates), do_one_replicate, clade_inf_by_county=clade_inf_by_county) |>
  purrr::list_rbind()
```

```{r}
sim_results$method <- factor(
  sim_results$method,
  levels = c("naive", "anesrake_cap2", "anesrake_cap5", "rake")
)
```

Plotting (the sampling distribution of) the resulting estimates
```{r}
ggplot(data = sim_results) +
  geom_boxplot(mapping = aes(x = clade, y = estimate, color = method)) +
  geom_point(
    data = as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    mapping = aes(x = clade, y = clade_props),
    size = 15,
    shape = "—"
  ) +
  theme_bw()
```

```{r}
ggplot(data = sim_results) +
  geom_violin(mapping = aes(x = clade, y = estimate, color = method)) +
  geom_point(
    data = as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    mapping = aes(x = clade, y = clade_props),
    size = 15,
    shape = "—"
  ) +
  theme_bw()
```

Here we'll look at mean absolute errors and mean squared errors of the estimates from each method for each clade:
```{r}
sim_results |>
  dplyr::left_join(
    as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    by = "clade"
  ) |>
  dplyr::mutate(
    error = estimate - clade_props,
    sq_error = error^2,
    abs_error = abs(error)
  ) |>
  dplyr::group_by(method, clade) |>
  dplyr::summarize(
    MSE = mean(sq_error),
    MAE = mean(abs_error)
  ) |>
  tidyr::pivot_longer(
    cols = c("MSE", "MAE"),
    names_to = "metric_name",
    values_to = "metric_value") |>
  ggplot() +
    geom_point(
      mapping = aes(x = clade, y = metric_value, color = method)
    ) +
    facet_wrap( ~ metric_name, ncol = 1, scales = "free_y") +
    theme_bw()
```

Here are the coverage rates of the two methods that produced confidence intervals:
```{r}
sim_results |>
  dplyr::filter(method %in% c("naive", "rake")) |>
  dplyr::left_join(
    as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    by = "clade"
  ) |>
  dplyr::mutate(
    cover = (clade_props >= ci_lower) & (clade_props <= ci_upper),
    interval_width = (ci_upper - ci_lower)
  ) |>
  dplyr::group_by(method, clade) |>
  dplyr::summarize(
    cover_rate = mean(cover),
    mean_interval_width = mean(interval_width)
  )
```


## Simulation 2: overrepresented county different from overall population

Now we modify the clade proportions for Worcester county to be different from the other counties:

```{r}
clade_props_by_county[pops$location == "Worcester", ] <- c(0.4, 0.2, 0.4)
cbind(pops['location'], clade_props_by_county)

pops$inf_count <- pops$pop * 5 / 100000 * 20
clade_inf_by_county <- round(clade_props_by_county * pops$inf_count)
print(cbind(pops["location"], clade_inf_by_county))

# the "realized" clade proportions in each county:
print(cbind(pops["location"],
            sweep(clade_inf_by_county,
                  1,
                  apply(clade_inf_by_county, 1, sum),
                  "/")))
```

Our goal is to obtain estimates of the proportion of all infections that are from each clade, state-wide:
```{r}
# population proportions to estimate
clade_counts <- apply(clade_inf_by_county, 2, sum)
clade_props <- clade_counts / sum(clade_counts)
print(clade_props)
```

```{r, results = 'hide'}
n_replicates <- 1000
set.seed(42)
sim_results_2 <- purrr::map(seq_len(n_replicates), do_one_replicate, clade_inf_by_county=clade_inf_by_county) |>
  purrr::list_rbind()
```

```{r}
sim_results_2$method <- factor(
  sim_results_2$method,
  levels = c("naive", "anesrake_cap2", "anesrake_cap5", "rake")
)
```

Plotting (the sampling distribution of) the resulting estimates
```{r}
ggplot(data = sim_results_2) +
  geom_boxplot(mapping = aes(x = clade, y = estimate, color = method)) +
  geom_point(
    data = as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    mapping = aes(x = clade, y = clade_props),
    size = 15,
    shape = "—"
  ) +
  theme_bw()
```

```{r}
ggplot(data = sim_results_2) +
  geom_violin(mapping = aes(x = clade, y = estimate, color = method)) +
  geom_point(
    data = as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    mapping = aes(x = clade, y = clade_props),
    size = 15,
    shape = "—"
  ) +
  theme_bw()
```

Here we'll look at mean absolute errors and mean squared errors of the estimates from each method for each clade:
```{r}
sim_results_2 |>
  dplyr::left_join(
    as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    by = "clade"
  ) |>
  dplyr::mutate(
    error = estimate - clade_props,
    sq_error = error^2,
    abs_error = abs(error)
  ) |>
  dplyr::group_by(method, clade) |>
  dplyr::summarize(
    MSE = mean(sq_error),
    MAE = mean(abs_error)
  ) |>
  tidyr::pivot_longer(
    cols = c("MSE", "MAE"),
    names_to = "metric_name",
    values_to = "metric_value") |>
  ggplot() +
    geom_point(
      mapping = aes(x = clade, y = metric_value, color = method)
    ) +
    facet_wrap( ~ metric_name, ncol = 1, scales = "free_y") +
    theme_bw()
```

Here are the coverage rates of the two methods that produced confidence intervals:
```{r}
sim_results_2 |>
  dplyr::filter(method %in% c("naive", "rake")) |>
  dplyr::left_join(
    as.data.frame(clade_props) |>
      dplyr::mutate(clade = c("A", "B", "C")),
    by = "clade"
  ) |>
  dplyr::mutate(
    cover = (clade_props >= ci_lower) & (clade_props <= ci_upper),
    interval_width = (ci_upper - ci_lower)
  ) |>
  dplyr::group_by(method, clade) |>
  dplyr::summarize(
    cover_rate = mean(cover),
    mean_interval_width = mean(interval_width)
  )
```


## Wrapping up

Take-aways about *point estimates*:

- In both simulation settings, the average or median estimate is better when using raking. In this simulation, raking is successful at correcting for the bias in the sampling.
- But raking increases the variability in estimates from different samples.
- Whether or not raking improved the average difference (or squared difference) between the estimate and the actual proportions depended on the simulation setting:
    - If Worcester county had roughly similar clade proportions as the state overall, the naive estimator had a better MAE and MSE
    - If Worcester county had very different clade proportions than the state overall, the raking estimator had a better MAE and MSE
- This is an example of a "bias-variance tradeoff": we can get reduced bias, but in order to do that we introduce more variability in estimates. We have to decide if the tradeoff is worth it.
- These results are likely sensitive to other things like the sample size.

Take-aways about *interval estimates*:

- As far as I can tell, there are not methods with statistical guarantees for getting interval estimates based on data gathered with this kind of non-designed sampling mechanism
- In the simulation setting where Worcester county had similar clade proportions to the state overall, the naive method had pretty good coverage rates
- In the setting where Worcester county had very different clade proportions than the state overall, the naive coverage rates were terrible; raking-based coverage rates were not great, but were better.


Misc. notes and open questions

 - If we want to choose between raking and naive methods, it seems like it will be important to know:
    - how much variation do we expect there to be among clade proportions in different locations?
    - what kinds of sample sizes will we typically be working with?
        - raking would benefit from larger sample sizes -- i think, especially in the locations with few samples
        - could we get any kind of minimum number of samples from each county?
 - Similarly, we might be able to improve the stability of the raking estimators by grouping the counties with fewer sequences into larger units (with the potential downside of losing information if we group together locations with different variant prevalences).
 - Are there other important ways to update the design of this simulation study to be more realistic?
 - Handling more than 1 covariate (e.g. race/ethnicity in addition to location)
    - this is technically possible
    - but it would make the problems with increased variability of the raked estimates even worse
 - What are plausible ways to get estimates based on samples collected across multiple weeks?
    - There are many options; my instinct would be to try to use raking in each week and then smooth with a model?
 - What are impacts of bias in which cases are selected for sequencing
    - by clade, e.g. maybe clade A is more prevalent but less severe, so fewer people get tested/sequenced
    - by other factors that may be associated with clade but are not controlled for in the analysis?
 - ...
